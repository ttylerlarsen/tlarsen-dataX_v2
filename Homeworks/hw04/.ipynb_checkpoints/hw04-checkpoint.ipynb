{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-X Spring 2018: Homework 04\n",
    "\n",
    "### Entropy, Natural Language Processing, SQL\n",
    "\n",
    "\n",
    "Write a sentence describing this assignment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1.1:**\n",
    "\n",
    "| HasJob   |                                       |\n",
    "| -------- |:-------------------------------------:|\n",
    "| HasJob=1 | P(Defaulter=1)     | $ \\frac{2}{5} $  |\n",
    "| HasJob=1 | P(Defaulter=0)     | $ \\frac{3}{5} $  |\n",
    "| HasJob=0 | P(Defaulter=1)     | $ \\frac{2}{3} $  |\n",
    "| HasJob=0 | P(Defaulter=0)     | $ \\frac{1}{3} $  |\n",
    "\n",
    "\n",
    "| HasFamily   |                                       |\n",
    "| ----------- |:-------------------------------------:|\n",
    "| HasFamily=1 | P(Defaulter=1)     | $ \\frac{1}{4} $  |\n",
    "| HasFamily=1 | P(Defaulter=0)     | $ \\frac{3}{4} $  |\n",
    "| HasFamily=0 | P(Defaulter=1)     | $ \\frac{3}{4} $  |\n",
    "| HasFamily=0 | P(Defaulter=0)     | $ \\frac{1}{4} $  |\n",
    "\n",
    "| IsAbove30years   |                                       |\n",
    "| ---------------- |:-------------------------------------:|\n",
    "| IsAbove30years=1 | P(Defaulter=1)     | $ \\frac{1}{2} $  |\n",
    "| IsAbove30years=1 | P(Defaulter=0)     | $ \\frac{1}{2} $  |\n",
    "| IsAbove30years=0 | P(Defaulter=1)     | $ \\frac{1}{2} $  |\n",
    "| IsAbove30years=0 | P(Defaulter=0)     | $ \\frac{1}{2} $  |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 1.2:**\n",
    "\n",
    "$ H(S) = \\sum_x p(x)\\log_2\\Big(\\frac{1}{p(x)}\\Big),  $  \n",
    "$ H(S) = p(A)\\log_2\\Big(\\frac{1}{p(A)}\\Big) + p(B)\\log_2\\Big(\\frac{1}{p(B)}\\Big) + p(C)\\log_2\\Big(\\frac{1}{p(x)}\\Big) $  \n",
    "$ H(S) = 0.7\\log_2(7) + 0.2\\log_2(2) + 0.1\\log_2(1) $  \n",
    "$ H(S) = 1.568 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a word2vec model on the corpus consisting of the text in the novel  Pride and Prejudice.  \n",
    "- DATA: Here is the csv file of the list of sentences in the novel. Here is the Wikipedia description of the novel. \n",
    "- PREPROCESS and MODEL: Preprocess the sentences to generate a data set with cleaned sentences (you can use the same cleaning procedure as shown in class). Use the cleaned sentences with word tokens as a corpus for training your Word2vec model, using the gensim package (you can use any hyperparameter setting).  \n",
    "- VISUALIZATION: Once you train the model, visualize the PCA decomposition of the word vectors.  \n",
    "- EVALUATION of the Trained Model:  Come up with five intrinsic evaluations of your trained model using methods like  most_similar( ), similarity( ), doesnt_match( ),  etc. For example one evaluation can be - model.similarity('elizabeth','girl').  \n",
    "  \n",
    "Note: Since the corpus is very small, be aware that your model vocabulary is very limited. You can increase the number of training iterations to improve embedding quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tylerlarsen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tylerlarsen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "#  >>> import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/prideNprejudice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cleaner(review,lemmatize=True,stem=False):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "\n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    #2. Remove punctuation\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "    \n",
    "    #3. Tokenize into words (all lower case)\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #4.Set stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    clean_review=[]\n",
    "    for word in review:\n",
    "        if word not in eng_stopwords:\n",
    "            if lemmatize is True:\n",
    "                word=wnl.lemmatize(word)\n",
    "            elif stem is True:\n",
    "                if word == 'oed':\n",
    "                    continue\n",
    "                word=ps.stem(word)\n",
    "            clean_review.append(word)\n",
    "    return(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and tokenize data\n",
    "clean_data = []\n",
    "for sentence in data['sentences']:\n",
    "    clean_data.append(gensim.utils.simple_preprocess(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5370"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(738664, 1100760)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build vocabulary and train model\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 2   # ignore all words with total frequency lower than this                       \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10 \n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "        clean_data,\n",
    "        workers=num_workers,\n",
    "        size=num_features, \n",
    "        min_count = min_word_count,\n",
    "        window = context, \n",
    "        iter=28)\n",
    "\n",
    "model.train(clean_data, total_examples=len(clean_data), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(738234, 1100760)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build vocabulary and train model\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # ignore all words with total frequency lower than this                       \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10 \n",
    "model = gensim.models.Word2Vec(\n",
    "        clean_data,\n",
    "        size=300,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10,\n",
    "        iter=28)\n",
    "\n",
    "model.train(clean_data, total_examples=len(clean_data), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerlarsen/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39407606921647803"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('elizabeth','girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('family','lydia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
